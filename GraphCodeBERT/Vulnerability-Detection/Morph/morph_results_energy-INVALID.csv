Tokenizer,Vocab Size,Num Hidden Layers,Hidden Size,Hidden Act,Hidden Dropout Prob,Intermediate Size,Num Attention Heads,Attention Probs Dropout Prob,Max Sequence Length,Position Embedding Type,Learning Rate,Batch Size,Size,Accuracy,FLOPS,Flips
3,3965,7,15,4,0.4,3043,5,0.4,276,3,2,3,3.0072712598445883,0.5406295754026355,26472.0,0
3,3304,8,16,1,0.2,2587,4,0.2,272,2,3,3,2.95519731936558,0.5534407027818448,17551.0,88
3,3857,7,15,1,0.2,2934,5,0.2,512,2,3,2,2.963314311570897,0.5966325036603221,54399.0,271
3,3633,7,18,1,0.2,2988,6,0.2,512,2,3,2,3.2160207837219557,0.5915080527086384,57798.0,257
3,4820,10,14,2,0.2,2472,7,0.2,504,3,3,2,3.0768932368136435,0.5406295754026355,143334.0,0
3,37098,2,16,2,0.2,2554,4,0.2,506,3,3,1,3.1174634591319546,0.5977306002928258,13610.0,298
3,1286,8,45,1,0.2,827,1,0.2,510,3,3,3,2.8931867450941717,0.58601756954612,88110.0,207
3,7974,5,20,1,0.4,2365,4,0.4,256,3,3,1,2.5855103486587714,0.5867496339677891,8769.0,317
4,3628,2,36,3,0.4,2884,6,0.4,504,1,3,3,2.3610320869371844,0.5761346998535871,8273.0,334
3,3717,7,16,3,0.4,2952,4,0.4,298,2,3,3,2.9934959101531096,0.5406295754026355,15391.0,0
3,4876,7,15,3,0.2,3064,5,0.2,501,3,3,2,3.0222711892035443,0.5775988286969254,44899.0,273
4,3842,8,16,1,0.2,2499,4,0.2,509,3,3,3,2.858659521129725,0.5530746705710102,83625.0,85
3,3849,7,15,3,0.4,2909,5,0.4,503,2,2,2,2.9740964069656313,0.5874816983894583,65911.0,284
3,3862,6,20,4,0.2,2415,5,0.2,505,2,2,2,2.68973870574894,0.6032210834553441,70339.0,255
3,4368,2,36,3,0.3,2889,6,0.3,501,2,3,3,2.732890558975816,0.599194729136164,7647.0,300
4,1103,2,138,3,0.4,862,1,0.4,490,1,3,2,3.2993830053116326,0.5728404099560761,0.0,260
3,5076,7,18,1,0.3,2454,6,0.3,511,2,2,3,2.9612577205841957,0.5915080527086384,69441.0,255
4,3643,1,163,1,0.3,158,1,0.3,260,2,3,2,3.6476218176791817,0.5834553440702782,1041.0,227
3,3737,6,20,1,0.4,2509,4,0.4,506,2,2,1,3.0334086137817833,0.6083455344070278,63914.0,323
3,3812,7,20,1,0.3,2477,5,0.3,505,3,2,2,3.1595819172444575,0.6076134699853587,68574.0,281
